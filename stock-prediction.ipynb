{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Just a heads up! This system was built on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-02T16:58:01.890013Z",
     "iopub.status.busy": "2025-11-02T16:58:01.888820Z",
     "iopub.status.idle": "2025-11-02T16:58:01.912911Z",
     "shell.execute_reply": "2025-11-02T16:58:01.911639Z",
     "shell.execute_reply.started": "2025-11-02T16:58:01.889919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/kaggle/input/stock-market-data/stock_market_data/sp500\")[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:18:18.115564Z",
     "iopub.status.busy": "2025-11-03T13:18:18.114619Z",
     "iopub.status.idle": "2025-11-03T13:18:32.423166Z",
     "shell.execute_reply": "2025-11-03T13:18:32.421878Z",
     "shell.execute_reply.started": "2025-11-03T13:18:18.115528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = \"/kaggle/input/stock-market-data/stock_market_data/sp500/csv\"\n",
    "\n",
    "#List all CSV files\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "print(\"Total CSV files found:\", len(files))\n",
    "\n",
    "#Load and combine them all\n",
    "df_list = []\n",
    "for file in files:\n",
    "    temp = pd.read_csv(os.path.join(data_path, file))\n",
    "    temp['Ticker'] = file.replace('.csv', '')\n",
    "    df_list.append(temp)\n",
    "\n",
    "#Merge all tickers into one DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:21.546843Z",
     "iopub.status.busy": "2025-11-03T13:19:21.546531Z",
     "iopub.status.idle": "2025-11-03T13:19:25.351393Z",
     "shell.execute_reply": "2025-11-03T13:19:25.350313Z",
     "shell.execute_reply.started": "2025-11-03T13:19:21.546820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Basic cleanup\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Date', 'Close'])\n",
    "df = df.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "#Feature engineering\n",
    "def add_features(data):\n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "    data['MA_7'] = data['Close'].rolling(7).mean()\n",
    "    data['MA_21'] = data['Close'].rolling(21).mean()\n",
    "    data['Volatility'] = data['Return'].rolling(21).std()\n",
    "    data['Momentum'] = data['Close'] / data['MA_7']\n",
    "    return data\n",
    "\n",
    "# Apply features by ticker \n",
    "df = df.groupby('Ticker', group_keys=False).apply(add_features)\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"After feature engineering\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:30.750406Z",
     "iopub.status.busy": "2025-11-03T13:19:30.748769Z",
     "iopub.status.idle": "2025-11-03T13:19:31.461717Z",
     "shell.execute_reply": "2025-11-03T13:19:31.460664Z",
     "shell.execute_reply.started": "2025-11-03T13:19:30.750366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_ticker = 'AAPL'\n",
    "apple = df[df['Ticker'] == sample_ticker]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(apple['Date'], apple['Close'], label='Close')\n",
    "plt.plot(apple['Date'], apple['MA_21'], label='MA_21')\n",
    "plt.title(f\"{sample_ticker} Close vs MA_21\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:38.831570Z",
     "iopub.status.busy": "2025-11-03T13:19:38.831234Z",
     "iopub.status.idle": "2025-11-03T13:19:38.837843Z",
     "shell.execute_reply": "2025-11-03T13:19:38.836814Z",
     "shell.execute_reply.started": "2025-11-03T13:19:38.831545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "print(\"wooohooo!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:41.746032Z",
     "iopub.status.busy": "2025-11-03T13:19:41.745670Z",
     "iopub.status.idle": "2025-11-03T13:19:42.772337Z",
     "shell.execute_reply": "2025-11-03T13:19:42.771367Z",
     "shell.execute_reply.started": "2025-11-03T13:19:41.746007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"Returns\"] = df[\"Close\"].pct_change()\n",
    "df[\"LogReturns\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df[\"MA_7\"] = df[\"Close\"].rolling(window=7).mean()\n",
    "df[\"MA_30\"] = df[\"Close\"].rolling(window=30).mean()\n",
    "df[\"Volatility_7\"] = df[\"Returns\"].rolling(window=7).std()\n",
    "df[\"Momentum\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "\n",
    "# Fix log issues and drop NaNs (no warnings)\n",
    "df[\"LogReturns\"] = df[\"LogReturns\"].replace([np.inf, -np.inf], np.nan)\n",
    "df.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:48.768575Z",
     "iopub.status.busy": "2025-11-03T13:19:48.768236Z",
     "iopub.status.idle": "2025-11-03T13:19:50.683792Z",
     "shell.execute_reply": "2025-11-03T13:19:50.682587Z",
     "shell.execute_reply.started": "2025-11-03T13:19:48.768552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Target column predict next-day closing price\n",
    "df[\"Target\"] = df[\"Close\"].shift(-1)\n",
    "# Create a binary target for classification\n",
    "df[\"Direction\"] = (df[\"Target\"] > df[\"Close\"]).astype(int)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "features = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'Returns', 'LogReturns', 'MA_7', 'MA_30', 'Volatility_7', 'Momentum'\n",
    "]\n",
    "X = df[features]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Normalizing features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "y_dir = df[\"Direction\"]\n",
    "\n",
    "X_train_dir, X_test_dir, y_train_dir, y_test_dir = train_test_split(\n",
    "    X_scaled, y_dir, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Data ready:\")\n",
    "print(\"Train shape\", X_train.shape)\n",
    "print(\"Test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:19:53.704998Z",
     "iopub.status.busy": "2025-11-03T13:19:53.704576Z",
     "iopub.status.idle": "2025-11-03T13:19:53.715504Z",
     "shell.execute_reply": "2025-11-03T13:19:53.714086Z",
     "shell.execute_reply.started": "2025-11-03T13:19:53.704921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Define the checkpoint filename. Use a filename that's easy to spot on Kaggle.\n",
    "checkpoint_filepath = 'dnn_model_checkpoint.h5' \n",
    "\n",
    "#Create the ModelCheckpoint callback instance\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, # Set to False to save the whole model (architecture + weights + optimizer state)\n",
    "    monitor='val_loss',      # Metric to monitor (e.g., validation loss)\n",
    "    mode='min',              # Save when the monitored metric is minimized\n",
    "    save_best_only=True,     # Only save a file if the current epoch is better than all previous ones\n",
    "    verbose=1                # Show a message when a model is saved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:20:42.313404Z",
     "iopub.status.busy": "2025-11-03T13:20:42.313089Z",
     "iopub.status.idle": "2025-11-03T13:25:38.348699Z",
     "shell.execute_reply": "2025-11-03T13:25:38.346834Z",
     "shell.execute_reply.started": "2025-11-03T13:20:42.313383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    layers.BatchNormalization(),                                             \n",
    "    layers.Dropout(0.3),                                                     \n",
    "    layers.Dense(64, activation='relu'),                                     \n",
    "    layers.Dense(32, activation='relu'),                                     \n",
    "    layers.Dense(1, activation='linear')                                     \n",
    "])  \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-03T13:25:38.353237Z",
     "iopub.status.idle": "2025-11-03T13:25:38.353541Z",
     "shell.execute_reply": "2025-11-03T13:25:38.353415Z",
     "shell.execute_reply.started": "2025-11-03T13:25:38.353403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Base estimators to be tuned via RandomizedSearchCV\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_base = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "# Hyperparameter search spaces (keep small if runtime is an issue)\n",
    "rf_param_dist = {\n",
    "    \"n_estimators\": [200, 400, 800],\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "xgb_param_dist = {\n",
    "    \"n_estimators\": [400, 800, 1200],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "    \"reg_alpha\": [0.0, 0.1, 0.5],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "# Randomized search objects (not fitted yet)\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_base,\n",
    "    rf_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    xgb_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured for RF and XGB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-03T13:20:26.381041Z",
     "iopub.status.idle": "2025-11-03T13:20:26.381352Z",
     "shell.execute_reply": "2025-11-03T13:20:26.381233Z",
     "shell.execute_reply.started": "2025-11-03T13:20:26.381219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter search on the training set\n",
    "rf_search.fit(X_train, y_train)\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "# Use the best estimators for the rest of the pipeline\n",
    "rf = rf_search.best_estimator_\n",
    "xgb = xgb_search.best_estimator_\n",
    "\n",
    "print(\"Best RF params:\", rf_search.best_params_)\n",
    "print(\"Best XGB params:\", xgb_search.best_params_)\n",
    "\n",
    "# Quick sanity-check predictions (results not used later)\n",
    "_ = rf.predict(X_test)\n",
    "_ = xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-03T13:20:26.387342Z",
     "iopub.status.idle": "2025-11-03T13:20:26.387710Z",
     "shell.execute_reply": "2025-11-03T13:20:26.387520Z",
     "shell.execute_reply.started": "2025-11-03T13:20:26.387504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dnn_train = model.predict(X_train).flatten()\n",
    "dnn_test = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:38:25.206833Z",
     "iopub.status.busy": "2025-11-02T19:38:25.206241Z",
     "iopub.status.idle": "2025-11-02T19:42:14.908390Z",
     "shell.execute_reply": "2025-11-02T19:42:14.906992Z",
     "shell.execute_reply.started": "2025-11-02T19:38:25.206799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_train = rf.predict(X_train)\n",
    "xgb_train = xgb.predict(X_train)\n",
    "dnn_train = model.predict(X_train).flatten()\n",
    "\n",
    "# Base predictions on TEST data\n",
    "rf_test = rf.predict(X_test)\n",
    "xgb_test = xgb.predict(X_test)\n",
    "dnn_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Combine into meta-features\n",
    "X_meta_train = np.column_stack((rf_train, xgb_train, dnn_train))\n",
    "X_meta_test  = np.column_stack((rf_test, xgb_test, dnn_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:47:54.498829Z",
     "iopub.status.busy": "2025-11-02T19:47:54.498423Z",
     "iopub.status.idle": "2025-11-02T19:47:55.285422Z",
     "shell.execute_reply": "2025-11-02T19:47:55.284295Z",
     "shell.execute_reply.started": "2025-11-02T19:47:54.498803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "meta = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "# or meta = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05)\n",
    "meta.fit(X_meta_train, y_train)\n",
    "\n",
    "meta_preds = meta.predict(X_meta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:47:58.608681Z",
     "iopub.status.busy": "2025-11-02T19:47:58.607583Z",
     "iopub.status.idle": "2025-11-02T19:47:58.630318Z",
     "shell.execute_reply": "2025-11-02T19:47:58.629135Z",
     "shell.execute_reply.started": "2025-11-02T19:47:58.608641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, meta_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, meta_preds))\n",
    "r2 = r2_score(y_test, meta_preds)\n",
    "\n",
    "print(f\"Super Ensemble MAE: {mae:.4f}\")\n",
    "print(f\"Super Ensemble RMSE: {rmse:.4f}\")\n",
    "print(f\"Super Ensemble R^2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:48:02.504476Z",
     "iopub.status.busy": "2025-11-02T19:48:02.503778Z",
     "iopub.status.idle": "2025-11-02T19:48:05.073311Z",
     "shell.execute_reply": "2025-11-02T19:48:05.072121Z",
     "shell.execute_reply.started": "2025-11-02T19:48:02.504441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test.values, label='Actual', alpha=0.8)\n",
    "plt.plot(meta_preds, label='Super Ensemble Predicted', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title(\"Super Stock Prediction Algorithm (SSPA) Results\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Now we get to the reallly fun part, turning our model into a real stock predictor!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:48:09.903442Z",
     "iopub.status.busy": "2025-11-02T19:48:09.903034Z",
     "iopub.status.idle": "2025-11-02T19:48:09.911869Z",
     "shell.execute_reply": "2025-11-02T19:48:09.910240Z",
     "shell.execute_reply.started": "2025-11-02T19:48:09.903414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Applies the same feature engineering steps.\"\"\"\n",
    "    df[\"Returns\"] = df[\"Close\"].pct_change()\n",
    "    df[\"LogReturns\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "    df[\"MA_7\"] = df[\"Close\"].rolling(window=7).mean()\n",
    "    df[\"MA_30\"] = df[\"Close\"].rolling(window=30).mean()\n",
    "    df[\"Volatility_7\"] = df[\"Returns\"].rolling(window=7).std()\n",
    "    df[\"Momentum\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "    df[\"LogReturns\"] = df[\"LogReturns\"].replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:48:14.183509Z",
     "iopub.status.busy": "2025-11-02T19:48:14.183164Z",
     "iopub.status.idle": "2025-11-02T19:48:14.191447Z",
     "shell.execute_reply": "2025-11-02T19:48:14.189900Z",
     "shell.execute_reply.started": "2025-11-02T19:48:14.183486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_next_close(new_data, scaler, rf, xgb, dnn, meta):\n",
    "    df = preprocess_data(new_data.copy())\n",
    "    \n",
    "    features = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "        'Returns', 'LogReturns', 'MA_7', 'MA_30', 'Volatility_7', 'Momentum'\n",
    "    ]\n",
    "    X_new = df[features]\n",
    "    X_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    # Base model predictions\n",
    "    rf_pred = rf.predict(X_scaled)\n",
    "    xgb_pred = xgb.predict(X_scaled)\n",
    "    dnn_pred = dnn.predict(X_scaled).flatten()\n",
    "    \n",
    "    # Combine\n",
    "    X_meta = np.column_stack((rf_pred, xgb_pred, dnn_pred))\n",
    "    final_pred = meta.predict(X_meta)\n",
    "    \n",
    "    return final_pred[-1]  # Return latest prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:48:19.717334Z",
     "iopub.status.busy": "2025-11-02T19:48:19.716943Z",
     "iopub.status.idle": "2025-11-02T19:48:20.049896Z",
     "shell.execute_reply": "2025-11-02T19:48:20.048598Z",
     "shell.execute_reply.started": "2025-11-02T19:48:19.717306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "latest_data = df.tail(100)  # last 100 rows\n",
    "predicted_close = predict_next_close(latest_data, scaler, rf, xgb, model, meta)\n",
    "print(f\"Predicted next closing price {predicted_close:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:48:48.834968Z",
     "iopub.status.busy": "2025-11-02T19:48:48.833508Z",
     "iopub.status.idle": "2025-11-02T19:48:48.840698Z",
     "shell.execute_reply": "2025-11-02T19:48:48.839593Z",
     "shell.execute_reply.started": "2025-11-02T19:48:48.834933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def get_latest_stock_data(symbol=\"AAPL\", period=\"3mo\", interval=\"1d\"):\n",
    "    data = yf.download(symbol, period=period, interval=interval)\n",
    "    data.reset_index(inplace=True)\n",
    "    data.rename(columns={\n",
    "        \"Open\": \"Open\", \"High\": \"High\", \"Low\": \"Low\",\n",
    "        \"Close\": \"Close\", \"Volume\": \"Volume\"\n",
    "    }, inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T19:50:40.368281Z",
     "iopub.status.busy": "2025-11-02T19:50:40.367887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_data = get_latest_stock_data(\"AAPL\")\n",
    "pred = predict_next_close(new_data, scaler, rf, xgb, model, meta)\n",
    "print(f\"Predicted Next Close for AAPL: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#streamlit_app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Super Stock Predictor\")\n",
    "symbol = st.text_input(\"Enter Stock Symbol\", \"AAPL\")\n",
    "\n",
    "if st.button(\"Predict Next Close\"):\n",
    "    data = get_latest_stock_data(symbol)\n",
    "    pred = predict_next_close(data, scaler, rf, xgb, model, meta)\n",
    "    st.success(f\"Predicted next close for {symbol}: ${pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Now we move to predicting DIRECTION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:26:02.242843Z",
     "iopub.status.busy": "2025-11-03T13:26:02.242262Z",
     "iopub.status.idle": "2025-11-03T13:26:02.302611Z",
     "shell.execute_reply": "2025-11-03T13:26:02.301065Z",
     "shell.execute_reply.started": "2025-11-03T13:26:02.242805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Base classifiers for search\n",
    "rf_cls_base = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_cls_base = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "rf_cls_param_dist = {\n",
    "    \"n_estimators\": [200, 400, 800],\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "xgb_cls_param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "rf_cls_search = RandomizedSearchCV(\n",
    "    rf_cls_base,\n",
    "    rf_cls_param_dist,\n",
    "    n_iter=15,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_cls_search = RandomizedSearchCV(\n",
    "    xgb_cls_base,\n",
    "    xgb_cls_param_dist,\n",
    "    n_iter=15,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# DNN for classification\n",
    "dnn_cls = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # sigmoid = binary output\n",
    "])\n",
    "\n",
    "dnn_cls.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T13:26:25.635497Z",
     "iopub.status.busy": "2025-11-03T13:26:25.635174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter search on classification models\n",
    "rf_cls_search.fit(X_train_dir, y_train_dir)\n",
    "xgb_cls_search.fit(X_train_dir, y_train_dir)\n",
    "\n",
    "rf_cls = rf_cls_search.best_estimator_\n",
    "xgb_cls = xgb_cls_search.best_estimator_\n",
    "\n",
    "print(\"Best RF_cls params:\", rf_cls_search.best_params_)\n",
    "print(\"Best XGB_cls params:\", xgb_cls_search.best_params_)\n",
    "\n",
    "# Train DNN classifier as before\n",
    "dnn_cls.fit(X_train_dir, y_train_dir, epochs=30, batch_size=64, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Base model predictions\n",
    "rf_pred_cls = rf_cls.predict_proba(X_test_dir)[:,1]\n",
    "xgb_pred_cls = xgb_cls.predict_proba(X_test_dir)[:,1]\n",
    "dnn_pred_cls = dnn_cls.predict(X_test_dir).flatten()\n",
    "\n",
    "X_meta_cls = np.column_stack((rf_pred_cls, xgb_pred_cls, dnn_pred_cls))\n",
    "y_meta_cls = y_test_dir\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "meta_cls = LogisticRegression()\n",
    "meta_cls.fit(X_meta_cls, y_meta_cls)\n",
    "\n",
    "final_preds_cls = meta_cls.predict_proba(X_meta_cls)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "acc = accuracy_score(y_meta_cls, (final_preds_cls > 0.5).astype(int))\n",
    "auc = roc_auc_score(y_meta_cls, final_preds_cls)\n",
    "\n",
    "print(f\"Direction accuracy {acc:.3f}\")\n",
    "print(f\"Direction AUC {auc:.3f}\")\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(y_meta_cls, (final_preds_cls > 0.5).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "acc = accuracy_score(y_meta_cls, (final_preds_cls > 0.5).astype(int))\n",
    "auc = roc_auc_score(y_meta_cls, final_preds_cls)\n",
    "\n",
    "print(f\"Direction Accuracy: {acc:.3f}\")\n",
    "print(f\"Direction AUC: {auc:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_meta_cls, (final_preds_cls > 0.5).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#GREATT now we have meta for next day price and meta_cls for next-day direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#So we can run\n",
    "predicted_close = predict_next_close(latest_data, scaler, rf, xgb, model, meta)\n",
    "predicted_dir = meta_cls.predict_proba(X_meta_cls[-1].reshape(1, -1))[0,1]\n",
    "\n",
    "direction = \"UP\" if predicted_dir > 0.5 else \"DOWN \"\n",
    "print(f\"Predicted next closing price {predicted_close:.2f}\")\n",
    "print(f\"Predicted direction {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_market_shocks(data, ticker=None,\n",
    "                         vol_window=21, vol_z=2.5,\n",
    "                         volume_window=21, volume_z=2.5,\n",
    "                         break_short=10, break_long=50, break_z=2.0):\n",
    "    df = data.copy()\n",
    "    if ticker is not None and 'Ticker' in df.columns:\n",
    "        df = df[df['Ticker'] == ticker].copy()\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    if 'Returns' not in df.columns:\n",
    "        raise ValueError(\"Expected 'Returns' column; run feature engineering first.\")\n",
    "\n",
    "    events = []\n",
    "\n",
    "    #Volatility spikes\\n",
    "    vol = df['Returns'].rolling(vol_window).std()\\n",
    "    vol_mean = vol.rolling(vol_window).mean()\n",
    "    vol_std = vol.rolling(vol_window).std()\n",
    "    vol_zscore = (vol - vol_mean) / vol_std\n",
    "    df['vol_zscore'] = vol_zscore\n",
    "    vol_spikes = df[vol_zscore > vol_z]\n",
    "    for _, row in vol_spikes.iterrows():\n",
    "        events.append({\n",
    "            'Date': row['Date'],\n",
    "            'Ticker': row.get('Ticker', None),\n",
    "            'type': 'VOLATILITY_SPIKE',\n",
    "            'severity': float(row['vol_zscore']),\n",
    "            'details': f\"vol_zscore={row['vol_zscore']:.2f}\"\n",
    "        })\n",
    "\n",
    "    #Heavy volume\\n",
    "    if 'Volume' in df.columns:\\n",
    "        volm_mean = df['Volume'].rolling(volume_window).mean()\n",
    "        volm_std = df['Volume'].rolling(volume_window).std()\n",
    "        volume_zscore = (df['Volume'] - volm_mean) / volm_std\n",
    "        df['volume_zscore'] = volume_zscore\n",
    "        volume_spikes = df[volume_zscore > volume_z]\n",
    "        for _, row in volume_spikes.iterrows():\n",
    "            events.append({\n",
    "                'Date': row['Date'],\n",
    "                'Ticker': row.get('Ticker', None),\n",
    "                'type': 'HEAVY_VOLUME',\n",
    "                'severity': float(row['volume_zscore']),\n",
    "                'details': f\"volume_zscore={row['volume_zscore']:.2f}\"\n",
    "            })\n",
    "\n",
    "    #Structural breaks (mean return shift)\\n",
    "    ret_short = df['Returns'].rolling(break_short).mean()\\n",
    "    ret_long = df['Returns'].rolling(break_long).mean()\n",
    "    diff = ret_short - ret_long\n",
    "    diff_std = diff.rolling(break_long).std()\n",
    "    break_zscore = diff / diff_std\n",
    "    df['break_zscore'] = break_zscore\n",
    "    breaks = df[break_zscore.abs() > break_z]\n",
    "    for _, row in breaks.iterrows():\n",
    "        events.append({\n",
    "            'Date': row['Date'],\n",
    "            'Ticker': row.get('Ticker', None),\n",
    "            'type': 'STRUCTURAL_BREAK',\n",
    "            'severity': float(abs(row['break_zscore'])),\n",
    "            'details': f\"break_zscore={row['break_zscore']:.2f}\"\n",
    "        })\n",
    "\n",
    "    if not events:\n",
    "        return pd.DataFrame(columns=['Date', 'Ticker', 'type', 'severity', 'details'])\n",
    "\n",
    "    events_df = pd.DataFrame(events)\n",
    "    events_df = events_df.sort_values('Date').reset_index(drop=True)\n",
    "    return events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "events = detect_market_shocks(df)\\n",
    "print(\"All market shocks (head):\")\\n",
    "print(events.head())\\n",
    "\\n",
    "aapl_shocks = detect_market_shocks(df, ticker=\"AAPL\")\\n",
    "print(\"AAPL shocks (head):\")\\n",
    "print(aapl_shocks.head())\\n",
    "\\n",
    "events_sensitive = detect_market_shocks(\\n",
    "    df,\\n",
    "    ticker=\"AAPL\",\\n",
    "    vol_z=2.0,\\n",
    "    volume_z=2.0,\\n",
    "    break_short=5,\\n",
    "    break_long=30,\\n",
    "    break_z=1.8,\\n",
    ")\\n",
    "print(\"AAPL shocks (sensitive, head):\")\\n",
    "print(events_sensitive.head())\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#More updates regarding \n",
    "# :Technical stuff like (RSI, MACD, etc coming soon)\\n",
    "# Inluding stuff lik Finnhub APIs\\n",
    "# Coming soon"
   ]
  }
],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1908,
     "sourceId": 17155,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1125174,
     "sourceId": 4861155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
